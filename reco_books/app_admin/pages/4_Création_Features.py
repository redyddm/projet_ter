import streamlit as st
import pandas as pd
import gensim
import os

from recommandation_de_livres.config import PROCESSED_DATA_DIR
from recommandation_de_livres.loaders.load_data import load_parquet
from recommandation_de_livres.iads.utils import save_df_to_csv, save_df_to_parquet
from recommandation_de_livres.iads.content_utils import combine_text
from recommandation_de_livres.iads.text_cleaning import nettoyage_leger, nettoyage_avance

# R√©duire les logs TensorFlow parasites
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

st.set_page_config(page_title="G√©n√©ration de Features", layout="wide", page_icon="üß†")

# -------------------------------
# Dataset choisi depuis accueil
# -------------------------------
DIR = st.session_state.get("DIR", None)

if not DIR:
    st.error("‚ö†Ô∏è Aucun dataset s√©lectionn√©. Retournez √† l'accueil.")
    st.stop()

dataset_path = PROCESSED_DATA_DIR / DIR / "content_dataset.parquet"
if not dataset_path.exists():
    st.error(f"Fichier introuvable : {dataset_path}")
    st.stop()

st.title(f"üß† G√©n√©ration des Features - {DIR}")

tab1, tab2, tab3 = st.tabs(["Sentence-BERT", "Word2Vec", "TF-IDF"])

# =====================================================
# ======== TAB 1 : Sentence-BERT ======================
# =====================================================
with tab1:
    st.subheader("üî§ Features Sentence-BERT")
    st.info("Nettoie et fusionne les textes pour un futur encodage avec Sentence-BERT.")

    if st.button("Lancer la g√©n√©ration SBERT"):
        progress = st.progress(0, text="Initialisation...")

        try:
            # 1. Chargement
            progress.progress(10, text="Chargement du dataset...")
            content_df = load_parquet(dataset_path)

            # 2. Fusion des textes
            progress.progress(30, text="Fusion des champs texte...")
            content_df['text_combined'] = content_df.progress_apply(combine_text, axis=1)

            # 3. Nettoyage l√©ger
            progress.progress(60, text="Nettoyage...")
            content_df['text_clean'] = content_df['text_combined'].progress_apply(nettoyage_leger)

            # 4. DataFrame features
            progress.progress(80, text="Cr√©ation du DataFrame...")
            features_df = pd.DataFrame({
                'item_id': content_df['item_id'],
                'text_clean': content_df['text_clean']
            })

            # 5. Sauvegarde
            output_path_csv = PROCESSED_DATA_DIR / DIR / "features_sbert.csv"
            output_path_parquet = PROCESSED_DATA_DIR / DIR / "features_sbert.parquet"
            output_path_csv.parent.mkdir(parents=True, exist_ok=True)

            save_df_to_csv(features_df, output_path_csv)
            save_df_to_parquet(features_df, output_path_parquet)

            progress.progress(100, text="Termin√© ‚úÖ")
            st.success("Features SBERT g√©n√©r√©es avec succ√®s !")

            # T√©l√©chargement direct
            st.download_button(
                label="T√©l√©charger CSV",
                data=features_df.to_csv(index=False).encode("utf-8"),
                file_name="features_sbert.csv",
                mime="text/csv",
            )

            st.download_button(
                label="T√©l√©charger Parquet",
                data=features_df.to_parquet(index=False),
                file_name="features_sbert.parquet",
                mime="application/octet-stream",
            )

        except Exception as e:
            st.error(f"Erreur : {str(e)}")

# =====================================================
# ======== TAB 2 : Word2Vec ===========================
# =====================================================
with tab2:
    st.subheader("üî§ Features Word2Vec")
    st.info("Nettoie, tokenise et pr√©pare les textes pour un entra√Ænement Word2Vec.")

    if st.button("Lancer la g√©n√©ration Word2Vec"):
        progress = st.progress(0, text="Initialisation...")

        try:
            # 1. Chargement
            progress.progress(10, text="Chargement du dataset...")
            content_df = load_parquet(dataset_path)

            # 2. Fusion des textes
            progress.progress(30, text="Fusion des champs texte...")
            content_df['text_combined'] = content_df.progress_apply(combine_text, axis=1)

            # 3. Nettoyage + tokenisation
            progress.progress(60, text="Tokenisation...")
            content_df['text_clean'] = content_df['text_combined'].progress_apply(gensim.utils.simple_preprocess)

            # 4. DataFrame features
            progress.progress(80, text="Cr√©ation du DataFrame...")
            features_df = pd.DataFrame({
                'item_id': content_df['item_id'],
                'text_clean': content_df['text_clean']
            })

            # 5. Sauvegarde
            output_path_csv = PROCESSED_DATA_DIR / DIR / "features_w2v.csv"
            output_path_parquet = PROCESSED_DATA_DIR / DIR / "features_w2v.parquet"
            output_path_csv.parent.mkdir(parents=True, exist_ok=True)

            save_df_to_csv(features_df, output_path_csv)
            save_df_to_parquet(features_df, output_path_parquet)

            progress.progress(100, text="Termin√© ‚úÖ")
            st.success("Features Word2Vec g√©n√©r√©es avec succ√®s !")

            # T√©l√©chargement direct
            st.download_button(
                label="T√©l√©charger CSV",
                data=features_df.to_csv(index=False).encode("utf-8"),
                file_name="features_w2v.csv",
                mime="text/csv",
            )

            st.download_button(
                label="T√©l√©charger Parquet",
                data=features_df.to_parquet(index=False),
                file_name="features_w2v.parquet",
                mime="application/octet-stream",
            )

        except Exception as e:
            st.error(f"Erreur : {str(e)}")

with tab3:
    st.subheader("üìä Features TF-IDF")
    st.info("Pr√©pare les textes (titres + auteurs) pour un entra√Ænement TF-IDF.")

    if st.button("Lancer la g√©n√©ration TF-IDF"):
        progress = st.progress(0, text="Initialisation...")

        try:
            # 1. Chargement
            progress.progress(10, text="Chargement du dataset...")
            content_df = load_parquet(dataset_path)

            # 2. Pr√©paration des textes (concat√©nation titre + auteurs)
            progress.progress(30, text="Pr√©paration des champs texte...")
            content_df['text_for_tfidf'] = content_df.progress_apply(
                lambda row: f"{row['title']} {row['authors']}".strip(), axis=1
            )

            # 3. Nettoyage avanc√©
            progress.progress(60, text="Nettoyage avanc√© des textes...")
            content_df['text_clean'] = content_df['text_for_tfidf'].progress_apply(nettoyage_avance)

            # 4. DataFrame features
            progress.progress(80, text="Cr√©ation du DataFrame...")
            features_df = pd.DataFrame({
                'item_id': content_df['item_id'],
                'text_clean': content_df['text_clean']
            })

            # 5. Sauvegarde
            output_path_csv = PROCESSED_DATA_DIR / DIR / "features_tfidf.csv"
            output_path_parquet = PROCESSED_DATA_DIR / DIR / "features_tfidf.parquet"
            output_path_csv.parent.mkdir(parents=True, exist_ok=True)

            save_df_to_csv(features_df, output_path_csv)
            save_df_to_parquet(features_df, output_path_parquet)

            progress.progress(100, text="Termin√© ‚úÖ")
            st.success("Features TF-IDF g√©n√©r√©es avec succ√®s !")

            # T√©l√©chargement direct
            st.download_button(
                label="T√©l√©charger CSV",
                data=features_df.to_csv(index=False).encode("utf-8"),
                file_name="features_tfidf.csv",
                mime="text/csv",
            )

            st.download_button(
                label="T√©l√©charger Parquet",
                data=features_df.to_parquet(index=False),
                file_name="features_tfidf.parquet",
                mime="application/octet-stream",
            )

        except Exception as e:
            st.error(f"Erreur : {str(e)}")
